{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from q2_sampler import svhn_sampler\n",
    "from q2_model import Critic, Generator\n",
    "from torch import optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "def lp_reg(x, y, critic):\n",
    "    \"\"\"\n",
    "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
    "\n",
    "    *** The notation used for the parameters follow the one from Petzka et al: https://arxiv.org/pdf/1709.08894.pdf\n",
    "    In other word, x are samples from the distribution mu and y are samples from the distribution nu. The critic is the\n",
    "    equivalent of f in the paper. Also consider that the norm used is the L2 norm. This is important to consider,\n",
    "    because we make the assumption that your implementation follows this notation when testing your function. ***\n",
    "\n",
    "    :param x: (FloatTensor) - shape: (batchsize x featuresize) - Samples from a distribution P.\n",
    "    :param y: (FloatTensor) - shape: (batchsize x featuresize) - Samples from a distribution Q.\n",
    "    :param critic: (Module) - torch module that you want to regularize.\n",
    "    :return: (FloatTensor) - shape: (1,) - Lipschitz penalty\n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    lambdas = torch.rand(batch_size, 1, 1, 1).to(x.device)\n",
    "    # lambdas = lambdas.expand(x.size())\n",
    "    \n",
    "    interpolation = lambdas * x + (1 - lambdas) * y\n",
    "    interpolation.requires_grad_(True)\n",
    "    # interpolation.retain_grad()\n",
    "\n",
    "    interp_logit = critic(interpolation)\n",
    "    # grad_output = torch.ones_like(interp_logit)\n",
    "    \n",
    "    gradient = torch.autograd.grad(\n",
    "        outputs=interp_logit,\n",
    "        inputs=interpolation,\n",
    "        grad_outputs=torch.ones_like(interp_logit),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(batch_size, -1)\n",
    "    zeros = torch.zeros(batch_size, 1)\n",
    "    grad_norm = gradient.norm(2, dim=1)\n",
    "\n",
    "    return torch.mean(torch.max((grad_norm - 1), zeros)** 2)\n",
    "\n",
    "\n",
    "\n",
    "def vf_wasserstein_distance(p, q, critic):\n",
    "    \"\"\"\n",
    "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
    "\n",
    "    *** The notation used for the parameters follow the one from Petzka et al: https://arxiv.org/pdf/1709.08894.pdf\n",
    "    In other word, x are samples from the distribution mu and y are samples from the distribution nu. The critic is the\n",
    "    equivalent of f in the paper. This is important to consider, because we make the assuption that your implementation\n",
    "    follows this notation when testing your function. ***\n",
    "\n",
    "    :param p: (FloatTensor) - shape: (batchsize x featuresize) - Samples from a distribution p.\n",
    "    :param q: (FloatTensor) - shape: (batchsize x featuresize) - Samples from a distribution q.\n",
    "    :param critic: (Module) - torch module used to compute the Wasserstein distance\n",
    "    :return: (FloatTensor) - shape: (1,) - Estimate of the Wasserstein distance\n",
    "    \"\"\"\n",
    "    vf_wasserstein_distance = torch.mean(critic(p)) - torch.mean(critic(q))\n",
    "    return vf_wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee163018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage of the code provided and recommended hyper parameters for training GANs.\n",
    "data_root = './'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_iter = 50000 # N training iterations\n",
    "n_critic_updates = 5 # N critic updates per generator update\n",
    "lp_coeff = 10 # Lipschitz penalty coefficient\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "lr = 1e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "z_dim = 100\n",
    "\n",
    "train_loader, valid_loader, test_loader = svhn_sampler(data_root, train_batch_size, test_batch_size)\n",
    "\n",
    "generator = Generator(z_dim=z_dim).to(device)\n",
    "critic = Critic().to(device)\n",
    "\n",
    "optim_critic = optim.Adam(critic.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optim_generator = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c19835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./train_32x32.mat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e273bd6297d14de79574a8db03a94a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182040794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./train_32x32.mat\n",
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./test_32x32.mat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dd856c3d4242c6a846eebb5217b328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64275384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMPLETE TRAINING PROCEDURE\n",
    "train_iter = iter(train_loader)\n",
    "valid_iter = iter(valid_loader)\n",
    "test_iter = iter(test_loader)\n",
    "for i in range(n_iter):\n",
    "    generator.train()\n",
    "    critic.train()\n",
    "    for _ in range(n_critic_updates):\n",
    "        try:\n",
    "            data = next(train_iter)[0].to(device)\n",
    "        except Exception:\n",
    "            train_iter = iter(train_loader)\n",
    "            data = next(train_iter)[0].to(device)\n",
    "        #####\n",
    "        # train the critic model here\n",
    "        # train on real\n",
    "        real_input = data.to(device=device, dtype=torch.float32)\n",
    "        output_real = critic(real_input)\n",
    "        loss_real = output_real.mean()\n",
    "        loss_real.backward(torch.FloatTensor([-1]).to(device=device))\n",
    "        # train on fake\n",
    "        noise = Variable(torch.randn(batch_size, 100).to(device=device))\n",
    "        fake_input = generator(noise)\n",
    "        output_fake = critic(fake_input.data)\n",
    "        loss_fake = output_fake.mean()\n",
    "        loss_fake.backward(torch.FloatTensor([1]).to(device=device)) \n",
    "        # train on gradient\n",
    "        gradient_penalty = lp_reg(real_input.data, fake_input.data,\n",
    "                                                critic)\n",
    "        gradient_penalty.backward(retain_graph=True)\n",
    "        optim_critic.step() \n",
    "        #####\n",
    "\n",
    "    #####\n",
    "    # train the generator model here\n",
    "    generator.zero_grad()\n",
    "\n",
    "    noise = Variable(torch.randn(batch_size, 100).to(device=device))\n",
    "    fake_input = generator(noise)\n",
    "    score_generator = critic(fake_input)\n",
    "    loss_gen = score_generator.mean()\n",
    "    loss_gen.backward(torch.FloatTensor([-1]).to(device=device))\n",
    "    optim_generator.step()\n",
    "    #####\n",
    "\n",
    "    # Save sample images \n",
    "    if i % 100 == 0:\n",
    "        z = torch.randn(64, z_dim, device=device)\n",
    "        imgs = generator(z)\n",
    "        save_image(imgs, f'imgs_{i}.png', normalize=True, value_range=(-1, 1))\n",
    "\n",
    "\n",
    "# COMPLETE QUALITATIVE EVALUATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9fa75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
